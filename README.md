# Big Data & Machine Learning Portfolio

-----

## Python for Data Science

For a data scientist, the **80/20 of Python is mastering data manipulation and visualization**. The vast majority of time in a machine learning project is spent cleaning, exploring, and shaping data. A deep fluency in libraries like Pandas for data wrangling and Matplotlib/Seaborn for exploration is the foundation upon which all effective models are built.

My Python skills are demonstrated throughout all projects, with a focus on clean, efficient, and readable code for data analysis.

### ðŸŒ± Beginner

  - [x] Core Python syntax and data structures (`list`, `dict`, `set`)
  - [x] Writing and using functions
  - [x] NumPy for numerical operations and array manipulation
  - [x] Basic file handling (reading CSVs)

### ðŸš§ Intermediate

  - [x] Pandas DataFrame manipulation (`.loc`, `.iloc`, filtering, sorting)
  - [x] Grouping and aggregating data with `.groupby()`
  - [x] Merging, joining, and concatenating DataFrames
  - [x] Data visualization with Matplotlib and Seaborn

### ðŸš€ Advanced

  - [ ] Advanced Pandas techniques (multi-indexing, `.pipe()`, managing memory)
  - [ ] Writing object-oriented code (`class`) for reusable components
  - [ ] Applying functions across DataFrames with `.apply()`
  - [ ] Basic API interaction with `requests`

-----

## Big Data with PySpark

The **80/20 principle for PySpark is understanding the DataFrame API and the concept of distributed, lazy evaluation**. Most large-scale data transformation tasks can be solved using a core set of transformations (`select`, `filter`, `withColumn`, `groupBy`) and actions (`show`, `count`, `write`). Grasping how Spark distributes these operations is the key to unlocking its power.

My big data skills are showcased in a **[Customer Churn Prediction project](https://www.google.com/search?q=./PySpark_Projects/Customer_Churn/)** where I processed a large customer dataset to engineer features for a predictive model.

### ðŸŒ± Beginner

  - [x] Understanding the Spark ecosystem (Driver, Executors)
  - [x] Initializing a `SparkSession`
  - [x] Reading and writing data (`.read.parquet()`, `.read.csv()`)
  - [x] Basic DataFrame operations (`.select()`, `.filter()`, `.show()`)

### ðŸš§ Intermediate

  - [x] Data transformations with `.withColumn()`
  - [x] Aggregations with `.groupBy().agg()`
  - [x] Handling missing data
  - [x] Creating basic User-Defined Functions (UDFs)

### ðŸš€ Advanced

  - [ ] Understanding lazy evaluation and the query plan
  - [ ] Performance tuning with caching (`.cache()`) and partitioning
  - [ ] Using Spark SQL to query DataFrames
  - [ ] Working with more complex data types (ArrayType, StructType)

-----

## Machine Learning & Data Mining

For most business applications, the **80/20 of machine learning is mastering the end-to-end workflow for classic supervised learning**. This means being an expert in data cleaning, robust feature engineering, training models like Logistic Regression and Random Forest, and evaluating them properly with cross-validation and relevant metrics (e.g., AUC-ROC, F1-score).

I applied these skills in a **[Credit Scoring Model project](https://www.google.com/search?q=./Machine_Learning_Projects/Credit_Scoring/)**, where I developed a model to predict the probability of loan default.

### ðŸŒ± Beginner

  - [x] Understanding Supervised vs. Unsupervised learning
  - [x] The train-test split methodology
  - [x] Training a basic model (e.g., `LogisticRegression`, `DecisionTreeClassifier`)
  - [x] Making predictions with `.predict()` and `.predict_proba()`
  - [x] Evaluating a model with basic metrics (accuracy)

### ðŸš§ Intermediate

  - [x] Feature Engineering (scaling, one-hot encoding, label encoding)
  - [x] Cross-Validation for robust model evaluation
  - [x] Advanced classification metrics (Precision, Recall, F1-Score, ROC AUC)
  - [x] Regression metrics (MSE, RMSE, R-squared)
  - [x] Training more complex models (e.g., `RandomForestClassifier`)

### ðŸš€ Advanced

  - [ ] Building reusable ML pipelines with `scikit-learn Pipeline`
  - [ ] Hyperparameter tuning with `GridSearchCV` or `RandomizedSearchCV`
  - [ ] Handling imbalanced datasets (e.g., SMOTE)
  - [ ] Using ensemble methods (Gradient Boosting, AdaBoost)
  - [ ] Feature importance and model interpretability (e.g., SHAP)
